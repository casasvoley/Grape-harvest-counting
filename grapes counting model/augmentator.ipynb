{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poeCA-m2Gypg",
        "outputId": "a80b2794-e771-4968-9d76-278d7aba0e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "609zVpu3m92_",
        "outputId": "523f0603-038b-4448-a9e6-9b230682bf17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Grape-harvest-counting'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 162 (delta 4), reused 94 (delta 1), pack-reused 60\u001b[K\n",
            "Receiving objects: 100% (162/162), 294.07 MiB | 32.99 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "Updating files: 100% (138/138), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/casasvoley/Grape-harvest-counting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT_kCNsTGcFF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUUmv2NqGcFH"
      },
      "outputs": [],
      "source": [
        "def read_annotations_yolo(annotation_txt):\n",
        "    with open(annotation_txt, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    annotations = []\n",
        "    for line in lines:\n",
        "        annotations.append([float(i) for i in line.strip().split(\" \")])\n",
        "        \n",
        "    return annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dShHI_uGcFH"
      },
      "outputs": [],
      "source": [
        "def save_annotations_yolo(annotations, output_file):\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for annotation in annotations:\n",
        "            line = \" \".join(str(i) for i in annotation)\n",
        "            f.write(line + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyELx-gvGcFH"
      },
      "outputs": [],
      "source": [
        "def apply_transformation(img, annotations, transform):\n",
        "    labels = [x[0] for x in annotations]\n",
        "    bboxes = [x[1:] for x in annotations]\n",
        "    \n",
        "    result = transform(image=img, bboxes=bboxes, labels=labels)\n",
        "    transformed_img = result[\"image\"]\n",
        "    transformed_annotations = [\n",
        "        [label, *box] for label, box in zip(result['labels'], result['bboxes']) if box[2] > 0 and box[3] > 0\n",
        "    ]\n",
        "    \n",
        "    return transformed_img, transformed_annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov3pCa-NGcFI"
      },
      "outputs": [],
      "source": [
        "def augment_dataset(input_folder, output_folder, num_output_images=1000):\n",
        "    transform = A.Compose([\n",
        "        A.Resize(512, 512),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        A.RandomRotate90(p=0.5),\n",
        "        A.Rotate(limit=10, p=0.5),\n",
        "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.5),\n",
        "        A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(0.3, 0.5), p=0.5),\n",
        "        A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "        A.GaussNoise(var_limit=(10, 50), mean=0, p=0.5),\n",
        "        A.GaussianBlur(p=0.5),\n",
        "        A.MotionBlur(p=0.5),\n",
        "        A.CLAHE(clip_limit=4.0, p=0.5),\n",
        "        A.Cutout(num_holes=8, max_h_size=16, max_w_size=16, p=0.5),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['labels'], min_area=0.001, min_visibility=0.001))\n",
        "\n",
        "\n",
        "    transform_original = A.Compose([\n",
        "        A.Resize(512, 512)\n",
        "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['labels'], min_area=0.001, min_visibility=0.001))\n",
        "\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    all_files = os.listdir(input_folder)\n",
        "    img_files = [f for f in all_files if f.endswith(\".jpg\")]\n",
        "    num_files = len(img_files)\n",
        "\n",
        "    for img_name in img_files:\n",
        "        img_path = os.path.join(input_folder, img_name)\n",
        "        txt_path = os.path.join(input_folder, f\"{img_name[:-4]}.txt\")\n",
        "\n",
        "        # Checks if the .txt file exists before continuing\n",
        "        if not os.path.exists(txt_path):\n",
        "            continue\n",
        "        \n",
        "        img = cv2.imread(img_path)\n",
        "        annotations = read_annotations_yolo(txt_path)\n",
        "\n",
        "        # Resize the original image and its annotations\n",
        "        img_resized, annotations_resized = apply_transformation(img, annotations, transform_original)\n",
        "\n",
        "        # Save the resized image and its annotations in the output folder\n",
        "        cv2.imwrite(os.path.join(output_folder, img_name), img_resized)\n",
        "        save_annotations_yolo(annotations_resized, os.path.join(output_folder, f\"{img_name[:-4]}.txt\"))\n",
        "       \n",
        "        # Loop that generates augmented images and saves them in the output folder\n",
        "        for i in range(num_output_images // num_files):\n",
        "            img_aug, annotations_aug = apply_transformation(img, annotations, transform)\n",
        "\n",
        "            new_img_name = f\"{img_name[:-4]}_aug_{i}.jpg\"\n",
        "            new_txt_name = f\"{img_name[:-4]}_aug_{i}.txt\"\n",
        "\n",
        "            cv2.imwrite(os.path.join(output_folder, new_img_name), img_aug)\n",
        "            save_annotations_yolo(annotations_aug, os.path.join(output_folder, new_txt_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XIZ1wl7GcFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_folder = \"/content/Grape-harvest-counting/datasets/grapes/train\"\n",
        "output_folder = \"/content/Grape-harvest-counting/datasets/grapes_augmented/train\"\n",
        "num_output_images = 300\n",
        "\n",
        "augment_dataset(input_folder, output_folder, num_output_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gumkVRYGcFK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import albumentations as A\n",
        "\n",
        "def improve_brightness_saturation(input_image_path, output_image_path):\n",
        "    \n",
        "    transform = A.Compose([\n",
        "        A.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(0.3, 0.5), p=1.0),\n",
        "        A.HueSaturationValue(sat_shift_limit=40, p=1.0),  # Aumenta la diferencia de color\n",
        "        A.ImageCompression(quality_lower=60, quality_upper=100, p=1.0),  # Mejora los detalles de la imagen\n",
        "\n",
        "    ])\n",
        "\n",
        "    img = cv2.imread(input_image_path)\n",
        "\n",
        "    img_transformed = transform(image=img)[\"image\"]\n",
        "\n",
        "    cv2.imwrite(output_image_path, img_transformed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m2Q-YkyGcFL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example \n",
        "input_folder = \"/content/Grape-harvest-counting/datasets/grapes/test\"\n",
        "output_folder = \"/content/Grape-harvest-counting/datasets/grapes_augmented/test\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "all_files = os.listdir(input_folder)\n",
        "img_files = [f for f in all_files if f.endswith(\".jpg\")]\n",
        "\n",
        "for img_name in img_files:\n",
        "    input_image_path = os.path.join(input_folder, img_name)\n",
        "    output_image_path = os.path.join(output_folder, img_name)\n",
        "    improve_brightness_saturation(input_image_path, output_image_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yolo_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
